---
title: "Part 4: Regression Models - Model 7"
author: "David Spellman"
date: "12/9/2021"
output: html_document
---

## Load packages
```{r, load_tidyverse}
library(tidyverse)
library(caret)
library(tidymodels)
```

## Read data

The code chunk below reads in the final project data.  

```{r, read_final_data}
df <- readr::read_csv("cs_1675_fall2021_finalproject.csv", col_names = TRUE)
```

The `readr::read_csv()` function displays the data types and column names associated with the data. However, a glimpse is shown below that reveals the number of rows and also shows some of the representative values for the columns.  

```{r, show_data_glimpse}
dft <- df %>% mutate(output=boot::logit(output))

bfm_df <- dft
bf_df <- bfm_df %>% select(-m)

efm_df <- dft %>% mutate(x5 = 1.0 - (x1+x2+x3+x4), w=x2/(x3+x4), z=(x1+x2)/(x4+x5), t=v1*v2)
ef_df <- efm_df %>% select(-m)

ef_df %>% glimpse()
```

```{r, build_data_frames}

classes <- (df$output < 0.33)
cdf <- df %>% mutate(Class=ifelse(classes, "good", "bad"))
cdf <- cdf %>% mutate(event=ifelse(classes, 1, 0))
cdf %>% glimpse()

cbfm_df <- bfm_df %>% mutate(Class=ifelse(classes, "good", "bad")) %>% select(-output)
cbf_df <- bf_df  %>% mutate(Class=ifelse(classes, "good", "bad")) %>% select(-output)
cefm_df <- efm_df  %>% mutate(Class=ifelse(classes, "good", "bad")) %>% select(-output)
cef_df <- ef_df  %>% mutate(Class=ifelse(classes, "good", "bad")) %>% select(-output)

```


You may use caret or tidymodels to handle the training, testing, and evaluation.
You must train and tune the following models:

```{r}
my_metrics <- metric_set(accuracy, roc_auc, mn_log_loss)
```

#Logistic regression:

###Neural network

```{r, make_stan_prepro_recipe}
bp_stan <- recipe(Class ~ ., data = cef_df) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
```

```{r, make_nnet_wflow_objects}
nnet_spec <- mlp(hidden_units = tune(), penalty = tune(), epochs = 2000) %>% 
  set_engine("nnet", MaxNWts = 2000, trace=FALSE) %>% 
  set_mode("classification")

nnet_grid <- crossing(hidden_units = c(3, 5, 10),
                      penalty = 10^(seq(-10, 1.75, length.out = 15)))

nnet_wflow <- workflow() %>% 
  add_model(nnet_spec) %>% 
  add_recipe(bp_stan)
```

```{r, setup_cv_folds}
set.seed(2021)
cv_folds <- vfold_cv(cef_df, v = 5, repeats = 3, strata = Class)

cv_folds
```

The neural network is trained and assessed in the code chunk below.  

```{r, train_assess_nnet_model_obj}
start_nnet <- Sys.time() 

set.seed(111)
nnet_p4_m7 <- tune_grid(
  nnet_wflow,
  resamples = cv_folds,
  grid = nnet_grid,
  metrics = my_metrics,
  save_pred = TRUE
)

finish_nnet <- Sys.time()

finish_nnet - start_nnet

nnet_p4_m7$.notes

```


```{r}

nnet_p4_m7$.notes[[1]]

nnet_p4_m7 %>% collect_metrics()

```

```{r}
nnet_p4_m7 %>% collect_predictions() %>% 
  conf_mat(truth = Class, estimate = .pred_class)
```

```{r}

nnet_p4_m7 %>% collect_predictions(summarize = FALSE) %>% 
  group_nest(id, id2) %>% 
  mutate(conf_mats = map(data, 
                         ~ yardstick::conf_mat(.x, truth = Class, estimate = .pred_class))) %>% 
  pluck("conf_mats") %>% 
  map_dfr( ~ as.data.frame(.x$table)) %>% 
  group_by(Prediction, Truth) %>% 
  summarise(Freq = mean(Freq, na.rm = TRUE),
            .groups = 'drop') %>% 
  mutate(Prediction = forcats::fct_rev(Prediction)) %>% 
  ggplot(mapping = aes(x = Truth, y = Prediction)) +
  geom_tile(mapping = aes(fill = Freq),
            color = 'black') +
  geom_text(mapping = aes(label = round(Freq,2)),
            color = 'white', size = 7.5) +
  theme_bw() +
  theme(legend.position = 'none')

```

```{r}

nnet_p4_m7 %>% collect_predictions(summarize = FALSE) %>% 
  roc_curve(Class, .pred_good) %>% 
  autoplot()

```

```{r}
nnet_p4_m7 %>% readr::write_rds("part_4_model_7.rds")
```

```{r}
re_load_p4_m7 <- readr::read_rds("part_4_model_7.rds")
re_load_p4_m7 %>% summary()
```


